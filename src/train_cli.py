#!/usr/bin/env python3
"""
Script CLI pour entraÃ®ner l'agent Q-Learning Snake sans interface graphique.
IdÃ©al pour des entraÃ®nements rapides ou sur des machines sans affichage.
"""

import argparse
import os
from snake_env import SnakeEnvironment
from q_learning_agent import QLearningAgent
from trainer import Trainer
from visualizer import Visualizer


def main():
    """Point d'entrÃ©e principal du script CLI."""
    
    parser = argparse.ArgumentParser(
        description="EntraÃ®ner un agent Q-Learning Ã  jouer au Snake",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    
    # ParamÃ¨tres d'entraÃ®nement
    parser.add_argument(
        "--episodes", "-e",
        type=int,
        default=1000,
        help="Nombre d'Ã©pisodes d'entraÃ®nement"
    )
    parser.add_argument(
        "--max-steps", "-s",
        type=int,
        default=200,
        help="Nombre maximum de pas par Ã©pisode"
    )
    
    # HyperparamÃ¨tres Q-Learning
    parser.add_argument(
        "--alpha", "-a",
        type=float,
        default=0.1,
        help="Taux d'apprentissage (learning rate)"
    )
    parser.add_argument(
        "--gamma", "-g",
        type=float,
        default=0.95,
        help="Facteur d'escompte (discount factor)"
    )
    parser.add_argument(
        "--epsilon",
        type=float,
        default=1.0,
        help="ProbabilitÃ© d'exploration initiale"
    )
    parser.add_argument(
        "--epsilon-decay",
        type=float,
        default=0.995,
        help="Facteur de dÃ©croissance d'epsilon"
    )
    parser.add_argument(
        "--epsilon-min",
        type=float,
        default=0.01,
        help="Valeur minimale d'epsilon"
    )
    
    # ParamÃ¨tres environnement
    parser.add_argument(
        "--width",
        type=int,
        default=40,
        help="Largeur de la grille"
    )
    parser.add_argument(
        "--height",
        type=int,
        default=40,
        help="Hauteur de la grille"
    )
    parser.add_argument(
        "--seed",
        type=int,
        default=None,
        help="Seed pour la reproductibilitÃ©"
    )
    
    # Options d'affichage
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Afficher les dÃ©tails d'entraÃ®nement"
    )
    parser.add_argument(
        "--log-interval",
        type=int,
        default=100,
        help="Intervalle d'affichage des logs (en Ã©pisodes)"
    )
    
    # Sauvegarde et visualisation
    parser.add_argument(
        "--save-agent",
        type=str,
        default="snake_agent.pkl",
        help="Chemin de sauvegarde de l'agent"
    )
    parser.add_argument(
        "--save-plot",
        type=str,
        default="training_results.png",
        help="Chemin de sauvegarde des graphiques"
    )
    parser.add_argument(
        "--no-plot",
        action="store_true",
        help="Ne pas gÃ©nÃ©rer de graphiques"
    )
    
    # Ã‰valuation
    parser.add_argument(
        "--evaluate",
        type=int,
        default=0,
        help="Nombre d'Ã©pisodes d'Ã©valuation aprÃ¨s l'entraÃ®nement (0 = pas d'Ã©valuation)"
    )
    
    args = parser.parse_args()
    
    print("=" * 70)
    print("ðŸ SNAKE Q-LEARNING - ENTRAÃŽNEMENT CLI")
    print("=" * 70)
    
    # CrÃ©er l'environnement
    print("\nðŸ“¦ Initialisation de l'environnement...")
    env = SnakeEnvironment(
        width=args.width,
        height=args.height,
        seed=args.seed
    )
    print(f"   Grille: {args.width}x{args.height}")
    
    # CrÃ©er l'agent
    print("\nðŸ¤– CrÃ©ation de l'agent Q-Learning...")
    agent = QLearningAgent(
        n_actions=4,
        learning_rate=args.alpha,
        discount_factor=args.gamma,
        epsilon=args.epsilon,
        epsilon_decay=args.epsilon_decay,
        epsilon_min=args.epsilon_min
    )
    print(f"   Î± (alpha):      {args.alpha}")
    print(f"   Î³ (gamma):      {args.gamma}")
    print(f"   Îµ (epsilon):    {args.epsilon} â†’ {args.epsilon_min}")
    print(f"   decay:          {args.epsilon_decay}")
    
    # CrÃ©er le trainer
    trainer = Trainer(env, agent)
    
    # EntraÃ®ner
    print("\nðŸŽ“ DÃ‰BUT DE L'ENTRAÃŽNEMENT")
    print("-" * 70)
    
    trainer.train(
        n_episodes=args.episodes,
        max_steps=args.max_steps,
        verbose=args.verbose,
        log_interval=args.log_interval
    )
    
    # RÃ©sumÃ©
    summary = trainer.get_training_summary()
    Visualizer.print_summary(summary)
    
    # Sauvegarder l'agent
    print(f"\nðŸ’¾ Sauvegarde de l'agent: {args.save_agent}")
    agent.save(args.save_agent)
    
    # GÃ©nÃ©rer les graphiques
    if not args.no_plot:
        print(f"\nðŸ“Š GÃ©nÃ©ration des graphiques: {args.save_plot}")
        metrics = trainer.get_metrics()
        Visualizer.plot_training_results(
            metrics,
            summary,
            save_path=args.save_plot,
            show=False
        )
    
    # Ã‰valuation
    if args.evaluate > 0:
        print(f"\nðŸŽ¯ Ã‰VALUATION ({args.evaluate} Ã©pisodes)")
        print("-" * 70)
        eval_results = trainer.evaluate(
            n_episodes=args.evaluate,
            max_steps=args.max_steps,
            verbose=True
        )
        
        if not args.no_plot:
            eval_plot_path = args.save_plot.replace(".png", "_eval.png")
            print(f"\nðŸ“Š GÃ©nÃ©ration des graphiques d'Ã©valuation: {eval_plot_path}")
            Visualizer.plot_evaluation_results(
                eval_results,
                save_path=eval_plot_path,
                show=False
            )
    
    print("\nâœ… TERMINÃ‰!")
    print("=" * 70)
    
    # Afficher les commandes pour visualiser
    print("\nðŸ“‹ Prochaines Ã©tapes:")
    print(f"   â€¢ Voir les graphiques: open {args.save_plot}")
    print(f"   â€¢ Charger l'agent: agent.load('{args.save_agent}')")
    if args.evaluate > 0:
        print(f"   â€¢ Voir l'Ã©valuation: open {args.save_plot.replace('.png', '_eval.png')}")
    print()


if __name__ == "__main__":
    main()
